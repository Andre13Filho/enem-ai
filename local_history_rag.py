#!/usr/bin/env python3
"""
Sistema RAG Local para Professor Eduardo - Hist√≥ria
Utiliza um √≠ndice FAISS pr√©-constru√≠do e baixado do Hugging Face.
"""

import streamlit as st
import os
import requests
from typing import Dict, List, Any, Optional

# LangChain imports
from langchain_community.vectorstores import FAISS
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from langchain.chains import ConversationalRetrievalChain
try:
    from langchain_community.memory import ConversationBufferMemory
except ImportError:
    from langchain.memory import ConversationBufferMemory
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun

# Groq para LLM
from groq import Groq

# Diret√≥rio para armazenar o √≠ndice FAISS
FAISS_INDEX_DIR = "faiss_index_history"

class GroqLLM(LLM):
    """LLM personalizado para DeepSeek R1 Distill via Groq"""
    
    api_key: str
    model_name: str = "deepseek-r1-distill-llama-70b"
    
    class Config:
        arbitrary_types_allowed = True
    
    def __init__(self, api_key: str, **kwargs):
        super().__init__(api_key=api_key, model_name="deepseek-r1-distill-llama-70b", **kwargs)
    
    @property
    def _llm_type(self) -> str:
        return "groq"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        try:
            # Cria uma nova inst√¢ncia do cliente a cada chamada para evitar cache corrompido
            client = Groq(api_key=self.api_key)
            response = client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=2048
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Erro na API: {str(e)}"

class LocalHistoryRAG:
    """Sistema RAG que carrega um √≠ndice FAISS remoto para Hist√≥ria."""
    
    def __init__(self):
        self.vectorstore = None
        self.retriever = None
        self.memory = None
        self.rag_chain = None
        self.embeddings = None
        self.is_initialized = False
        self.history_folder_path = FAISS_INDEX_DIR
        
        # O setup de embeddings foi movido para o m√©todo initialize()
        # para evitar carregamento pesado durante a importa√ß√£o.

    def _setup_embeddings(self, model_name: str):
        """Configura o modelo de embeddings do Hugging Face."""
        # Se os embeddings j√° estiverem carregados, n√£o faz nada
        if self.embeddings:
            return
        
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception as e:
            if 'st' in globals() and hasattr(st, 'error'):
                st.error(f"Falha ao carregar o modelo de embeddings: {e}")
            self.embeddings = None

    def _download_file(self, url: str, local_path: str):
        """Baixa um arquivo de uma URL para um caminho local."""
        try:
            with requests.get(url, stream=True) as r:
                r.raise_for_status()
                with open(local_path, 'wb') as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        f.write(chunk)
            print(f"‚úÖ Arquivo baixado: {local_path}")
            return True
        except requests.exceptions.RequestException as e:
            st.error(f"Erro de rede ao baixar {url}: {e}")
            print(f"‚ùå Erro de rede ao baixar {url}: {e}")
            return False
        
    def _ensure_faiss_index_is_ready(self) -> bool:
        """
        Garante que o √≠ndice FAISS esteja dispon√≠vel, baixando-o se necess√°rio.
        """
        os.makedirs(FAISS_INDEX_DIR, exist_ok=True)
        
        index_file = os.path.join(FAISS_INDEX_DIR, "index_history.faiss")
        pkl_file = os.path.join(FAISS_INDEX_DIR, "index_history.pkl")

        # Verifica se os dois arquivos j√° existem
        if os.path.exists(index_file) and os.path.exists(pkl_file):
            print("‚úÖ √çndice FAISS de hist√≥ria j√° existe localmente.")
            return True

        st.info("üì• Baixando √≠ndice de hist√≥ria do Hugging Face...")
        print("üì• Baixando √≠ndice de hist√≥ria do Hugging Face...")

        # URLs dos arquivos no Hugging Face
        faiss_url = "https://huggingface.co/Andre13Filho/rag_enem/resolve/main/index_history.faiss"
        pkl_url = "https://huggingface.co/Andre13Filho/rag_enem/resolve/main/index_history.pkl"

        # Baixa os dois arquivos
        faiss_success = self._download_file(faiss_url, index_file)
        pkl_success = self._download_file(pkl_url, pkl_file)

        if faiss_success and pkl_success:
            st.success("‚úÖ √çndice de hist√≥ria baixado com sucesso!")
            return True
        else:
            st.error("‚ùå Falha ao baixar os arquivos do √≠ndice de hist√≥ria.")
            # Limpa arquivos parciais em caso de falha
            if os.path.exists(index_file): 
                os.remove(index_file)
            if os.path.exists(pkl_file): 
                os.remove(pkl_file)
                return False
            
    def initialize(self, api_key: str) -> bool:
        """
        Inicializa o sistema: baixa o √≠ndice, carrega o vectorstore e cria a cadeia RAG.
        """
        if self.is_initialized:
            return True
            
        # 1. Garantir que o √≠ndice FAISS est√° dispon√≠vel
        if not self._ensure_faiss_index_is_ready():
            return False
            
        # 2. Carregar o Vectorstore FAISS (e configurar embeddings aqui)
        try:
            st.info("üìö Carregando base de conhecimento de hist√≥ria (FAISS)...")
            print("üìö Carregando base de conhecimento de hist√≥ria (FAISS)...")
            
            # Passo 2.1: Configurar embeddings ANTES de carregar o FAISS
            self._setup_embeddings(model_name="sentence-transformers/distiluse-base-multilingual-cased-v1")
            if not self.embeddings:
                st.error("Embeddings n√£o foram inicializadas. Abortando.")
                return False

            self.vectorstore = FAISS.load_local(
                FAISS_INDEX_DIR, 
                self.embeddings,
                allow_dangerous_deserialization=True # Necess√°rio para pkl
            )
            self.retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 5}
            )
            st.success("‚úÖ Base de conhecimento carregada.")
            print("‚úÖ Base de conhecimento carregada.")
        except Exception as e:
            st.error(f"Erro ao carregar o √≠ndice FAISS: {e}")
            print(f"‚ùå Erro ao carregar o √≠ndice FAISS: {e}")
            return False
    
        # 3. Criar a cadeia RAG
        try:
            st.info("üîó Criando a cadeia de conversa√ß√£o RAG...")
            print("üîó Criando a cadeia de conversa√ß√£o RAG...")
            
            self.memory = ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
                output_key="answer"
            )
            
            llm = GroqLLM(api_key=api_key)

            self.rag_chain = ConversationalRetrievalChain.from_llm(
                llm=llm,
                retriever=self.retriever,
                memory=self.memory,
                return_source_documents=True,
                output_key="answer",
            )
            
            # Adiciona o prompt personalizado para Hist√≥ria
            prompt_template = """Voc√™ √© o Professor Eduardo, especialista em hist√≥ria do ENEM. Responda como um professor para uma estudante de 17 anos chamada Sther.

üî• REGRAS DE FORMATA√á√ÉO HIST√ìRICA (CR√çTICO - SEMPRE SEGUIR):

1. **DELIMITADORES OBRIGAT√ìRIOS:**
   - Datas no meio do texto: $sua-data-aqui$
   - Per√≠odos em destaque: $$sua-periodo-aqui$$
   - NUNCA use \\text{Revolu√ß√£o} sozinho - sempre use $\\text{Revolu√ß√£o}$

2. **EXEMPLOS CORRETOS:**
   ‚úÖ A Revolu√ß√£o Francesa ocorreu em $1789$
   ‚úÖ O per√≠odo colonial: $$1500-1822$$
   ‚úÖ Para eventos: $$Independ√™ncia do Brasil = 1822$$

3. **COMANDOS LATEX ESSENCIAIS:**
   - Fra√ß√µes: $\\frac{numerador}{denominador}$
   - Ra√≠zes: $\\sqrt{x}$ ou $\\sqrt[n]{x}$
   - Texto em f√≥rmulas: $\\text{Revolu√ß√£o} = \\text{mudan√ßa social}$
   - Pot√™ncias: $s√©culo XIX$, $s√©culo XX$
   - √çndices: $1¬™ Guerra$, $2¬™ Guerra$

4. **SEMPRE INCLUIR:**
   - Explica√ß√£o passo-a-passo
   - Exemplos pr√°ticos com datas
   - Dicas para o ENEM
   - Analogias do cotidiano

5. **ESTILO DO PROFESSOR EDUARDO:**
   - Use analogias das s√©ries que a Sther gosta (FRIENDS, Big Bang Theory, etc.)
   - Seja did√°tico e paciente
   - Conecte eventos hist√≥ricos com exemplos pr√°ticos
   - Explique como Monica organizaria a cronologia hist√≥rica

6. **ANALOGIAS DAS S√âRIES POR T√ìPICO:**
   - **Hist√≥ria Antiga**: "Como Monica organizava suas lembran√ßas - cada civiliza√ß√£o antiga tinha sua organiza√ß√£o espec√≠fica!"
   - **Revolu√ß√µes**: "Pense nas revolu√ß√µes como quando Ross e Rachel se separavam - mudan√ßas que transformavam tudo!"
   - **Per√≠odos Hist√≥ricos**: "Como quando o grupo se reunia no Central Perk - cada √©poca tinha seu 'ponto de encontro' cultural!"
   - **Guerras**: "Lembra quando Chandler explicava conflitos? 'Could this BE more hist√≥rico?'"

Com base no CONTEXTO abaixo, responda √† PERGUNTA do aluno.
Se a resposta n√£o estiver no contexto, use seu conhecimento em hist√≥ria, mas mantenha o estilo.

CONTEXTO:
{context}

PERGUNTA: {question}

RESPOSTA (com datas bem formatadas e estilo do Professor Eduardo):
"""
            # Atualiza o prompt da cadeia
            if hasattr(self.rag_chain.combine_docs_chain, "llm_chain"):
                self.rag_chain.combine_docs_chain.llm_chain.prompt.template = prompt_template
            
            self.is_initialized = True
            st.success("‚úÖ Cadeia RAG criada e pronta para uso!")
            print("‚úÖ Cadeia RAG criada e pronta para uso!")
            return True

        except Exception as e:
            st.error(f"Erro ao criar a cadeia RAG: {e}")
            print(f"‚ùå Erro ao criar a cadeia RAG: {e}")
            return False
    
    def get_response(self, question: str) -> Dict[str, Any]:
        """Obt√©m uma resposta do sistema RAG."""
        if not self.rag_chain:
            return {"answer": "O sistema RAG n√£o foi inicializado corretamente."}
        
        try:
            return self.rag_chain({"question": question})
        except Exception as e:
            return {"answer": f"Erro ao processar a pergunta: {str(e)}"}
    
    def search_relevant_content(self, query: str, k: int = 3) -> List[Document]:
        """Busca por conte√∫do relevante no vectorstore."""
        if not self.vectorstore:
            return []
        
        try:
            return self.vectorstore.similarity_search(query, k=k)
        except Exception as e:
            print(f"Erro na busca de similaridade: {str(e)}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Retorna estat√≠sticas detalhadas do sistema RAG, incluindo uma amostra de documentos.
        """
        if not self.is_initialized or not self.vectorstore:
            return {
                "status": "N√£o Carregado",
                "total_documents": 0,
                "sample_documents": []
            }

        try:
            total_documents = self.vectorstore.index.ntotal
            
            # Pega uma amostra de metadados dos primeiros 5 documentos
            sample_docs_metadata = []
            docstore = self.vectorstore.docstore
            doc_ids = list(docstore._dict.keys())
            
            for i in range(min(5, len(doc_ids))):
                doc = docstore._dict[doc_ids[i]]
                if doc.metadata:
                    sample_docs_metadata.append(doc.metadata)

            # Extrai nomes de arquivos √∫nicos da amostra
            sample_files = sorted(list(set(
                meta.get("source", "Fonte Desconhecida") for meta in sample_docs_metadata
            )))

            return {
                "status": "Carregado",
                "total_documents": total_documents,
                "sample_documents": sample_files
            }
        except Exception as e:
            print(f"Erro ao obter estat√≠sticas do RAG: {e}")
            return {
                "status": "Erro na Leitura",
                "total_documents": 0,
                "sample_documents": [str(e)]
            }
    
    def clear_memory(self):
        """Limpa a mem√≥ria da conversa."""
        if self.memory:
            self.memory.clear()

_singleton_instance = None

def get_local_history_rag_instance():
    """
    Retorna uma inst√¢ncia √∫nica (singleton) do LocalHistoryRAG.
    Isso evita a inicializa√ß√£o no momento da importa√ß√£o.
    """
    global _singleton_instance
    if _singleton_instance is None:
        _singleton_instance = LocalHistoryRAG()
    return _singleton_instance 