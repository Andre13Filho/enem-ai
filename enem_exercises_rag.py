#!/usr/bin/env python3
"""
Sistema RAG para Exerc√≠cios do ENEM - Segundo Dia
Processa e indexa exerc√≠cios de matem√°tica e ci√™ncias da natureza do ENEM
"""

import os
import re
from pathlib import Path
from typing import List, Dict, Any, Optional
import streamlit as st

# Document processing
from pypdf import PdfReader
from docx import Document as DocxDocument

# LangChain imports
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document

class ENEMExercisesRAG:
    """Sistema RAG para exerc√≠cios do ENEM"""
    
    def __init__(self, enem_folder_path: str = "./Segundo dia"):
        self.enem_folder_path = Path(enem_folder_path)
        self.persist_directory = "./chroma_enem_exercises"
        self.embeddings = None
        self.vectorstore = None
        self.retriever = None
        self.documents = []
        
        # Configurar embeddings
        self._setup_embeddings()
    
    def _setup_embeddings(self):
        """Configura embeddings HuggingFace"""
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception as e:
            if 'st' in globals():
                st.error(f"Erro ao configurar embeddings: {str(e)}")
            else:
                print(f"Erro ao configurar embeddings: {str(e)}")
    
    def process_enem_documents(self) -> bool:
        """Processa todos os documentos do ENEM"""
        if not self.enem_folder_path.exists():
            st.error(f"Pasta do ENEM n√£o encontrada: {self.enem_folder_path}")
            return False
        
        try:
            print("üéØ Processando exerc√≠cios do ENEM...")
            
            # Busca todos os anos
            year_folders = [f for f in self.enem_folder_path.iterdir() if f.is_dir()]
            year_folders.sort(key=lambda x: x.name, reverse=True)  # Mais recentes primeiro
            
            processed_files = 0
            total_exercises = 0
            
            for year_folder in year_folders:
                year = year_folder.name
                print(f"üìÖ Processando ano {year}...")
                
                # Busca arquivos PDF de prova (n√£o gabarito)
                pdf_files = [f for f in year_folder.glob("*.pdf") if "gabarito" not in f.name.lower() and "gb" not in f.name.lower()]
                
                for pdf_file in pdf_files:
                    print(f"   üìÑ Processando: {pdf_file.name}")
                    
                    # Extrai exerc√≠cios do PDF
                    exercises = self._extract_exercises_from_pdf(pdf_file, year)
                    
                    if exercises:
                        self.documents.extend(exercises)
                        total_exercises += len(exercises)
                        processed_files += 1
                        print(f"   ‚úÖ {len(exercises)} exerc√≠cios extra√≠dos")
                    else:
                        print(f"   ‚ö†Ô∏è Nenhum exerc√≠cio encontrado")
            
            print(f"\nüìä Processamento conclu√≠do:")
            print(f"   üìÅ {processed_files} arquivos processados")
            print(f"   üéØ {total_exercises} exerc√≠cios extra√≠dos")
            
            if self.documents:
                # Cria vectorstore
                self._create_vectorstore()
                return True
            else:
                st.warning("Nenhum exerc√≠cio foi extra√≠do dos documentos")
                return False
                
        except Exception as e:
            st.error(f"Erro ao processar documentos do ENEM: {str(e)}")
            return False
    
    def _extract_exercises_from_pdf(self, pdf_path: Path, year: str) -> List[Document]:
        """Extrai exerc√≠cios individuais de um PDF do ENEM"""
        try:
            reader = PdfReader(str(pdf_path))
            full_text = ""
            
            # Extrai todo o texto do PDF
            for page in reader.pages:
                text = page.extract_text()
                if text.strip():
                    full_text += text + "\n"
            
            # Identifica exerc√≠cios por padr√µes
            exercises = self._parse_exercises_from_text(full_text, year, pdf_path.name)
            
            return exercises
            
        except Exception as e:
            print(f"Erro ao processar PDF {pdf_path}: {e}")
            return []
    
    def _parse_exercises_from_text(self, text: str, year: str, filename: str) -> List[Document]:
        """Identifica e separa exerc√≠cios individuais do texto"""
        exercises = []
        
        # Padr√µes para identificar quest√µes do ENEM
        # O ENEM geralmente usa "QUEST√ÉO XX" ou n√∫meros simples
        patterns = [
            r'QUEST√ÉO\s+(\d+)',  # QUEST√ÉO 136
            r'(?:^|\n)(\d+)\s*\.?\s*(?:[A-Z]|\()',  # 136. ou 136 seguido de texto
            r'(?:^|\n)(\d+)\s*(?=\w)',  # N√∫mero seguido de palavra
        ]
        
        # Tenta diferentes padr√µes para identificar quest√µes
        question_matches = []
        
        for pattern in patterns:
            matches = list(re.finditer(pattern, text, re.MULTILINE))
            if matches and len(matches) > 10:  # Se encontrou muitas quest√µes, usa esse padr√£o
                question_matches = matches
                break
        
        if not question_matches:
            # Se n√£o encontrou padr√£o claro, divide por p√°ginas/se√ß√µes
            return self._split_by_chunks(text, year, filename)
        
        # Extrai quest√µes individuais
        for i, match in enumerate(question_matches):
            question_num = match.group(1)
            start_pos = match.start()
            
            # Determina onde termina a quest√£o (in√≠cio da pr√≥xima ou fim do texto)
            if i + 1 < len(question_matches):
                end_pos = question_matches[i + 1].start()
            else:
                end_pos = len(text)
            
            question_text = text[start_pos:end_pos].strip()
            
            # Filtra quest√µes muito curtas ou muito longas
            if 100 < len(question_text) < 3000:
                # Identifica √°rea (matem√°tica ou ci√™ncias)
                area = self._identify_subject_area(question_text)
                
                # Cria documento para a quest√£o
                doc = Document(
                    page_content=question_text,
                    metadata={
                        "year": year,
                        "question_number": question_num,
                        "source_file": filename,
                        "subject_area": area,
                        "document_type": "exercise",
                        "topic": self._extract_topic_from_exercise(question_text)
                    }
                )
                exercises.append(doc)
        
        return exercises
    
    def _split_by_chunks(self, text: str, year: str, filename: str) -> List[Document]:
        """Divide texto em chunks quando n√£o consegue identificar quest√µes individuais"""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", ".", "!", "?", ";", ":", " ", ""]
        )
        
        chunks = text_splitter.split_text(text)
        documents = []
        
        for i, chunk in enumerate(chunks):
            if len(chunk.strip()) > 100:  # Filtra chunks muito pequenos
                doc = Document(
                    page_content=chunk,
                    metadata={
                        "year": year,
                        "chunk_number": i + 1,
                        "source_file": filename,
                        "subject_area": self._identify_subject_area(chunk),
                        "document_type": "exercise_chunk",
                        "topic": self._extract_topic_from_exercise(chunk)
                    }
                )
                documents.append(doc)
        
        return documents
    
    def _identify_subject_area(self, text: str) -> str:
        """Identifica se √© matem√°tica ou ci√™ncias da natureza"""
        text_lower = text.lower()
        
        # Palavras-chave para matem√°tica
        math_keywords = [
            'fun√ß√£o', 'equa√ß√£o', 'gr√°fico', 'geometria', 'trigonometria',
            'logaritmo', 'progress√£o', 'probabilidade', 'estat√≠stica',
            'derivada', 'integral', 'matriz', 'determinante', 'sistema',
            'polin√¥mio', 'raiz', 'v√©rtice', 'par√°bola', 'circunfer√™ncia'
        ]
        
        # Palavras-chave para ci√™ncias
        science_keywords = [
            'f√≠sica', 'qu√≠mica', 'biologia', 'c√©lula', '√°tomo', 'mol√©cula',
            'for√ßa', 'energia', 'velocidade', 'acelera√ß√£o', 'movimento',
            'rea√ß√£o', 'elemento', 'organismo', 'gen√©tica', 'evolu√ß√£o',
            'ecologia', 'termodin√¢mica', 'eletricidade', 'magnetismo'
        ]
        
        math_score = sum(1 for keyword in math_keywords if keyword in text_lower)
        science_score = sum(1 for keyword in science_keywords if keyword in text_lower)
        
        if math_score > science_score:
            return "Matem√°tica"
        elif science_score > math_score:
            return "Ci√™ncias da Natureza"
        else:
            return "Indeterminado"
    
    def _extract_topic_from_exercise(self, text: str) -> str:
        """Extrai t√≥pico espec√≠fico do exerc√≠cio"""
        text_lower = text.lower()
        
        # T√≥picos espec√≠ficos de matem√°tica
        math_topics = {
            "Geometria Plana": ['√°rea', 'per√≠metro', 'pol√≠gono', 'c√≠rculo', 'tri√¢ngulo', 'quadrado'],
            "Geometria Espacial": ['volume', 'cubo', 'esfera', 'cilindro', 'cone', 'pir√¢mide'],
            "Fun√ß√µes": ['fun√ß√£o', 'gr√°fico', 'dom√≠nio', 'imagem', 'f(x)', 'g(x)'],
            "An√°lise Combinat√≥ria": ['combinat√≥ria', 'permuta√ß√£o', 'arranjo', 'combina√ß√£o'],
            "Probabilidade": ['probabilidade', 'chance', 'sorteio', 'aleat√≥rio'],
            "Estat√≠stica": ['m√©dia', 'mediana', 'moda', 'desvio padr√£o'],
            "Trigonometria": ['seno', 'cosseno', 'tangente', 'trigonom√©trica'],
            "√Ålgebra": ['equa√ß√£o', 'express√£o', 'polin√¥mio', 'inequa√ß√£o']
        }
        
        # T√≥picos de Ci√™ncias da Natureza
        science_topics = {
            # F√≠sica
            "Mec√¢nica": ['for√ßa', 'movimento', 'energia', 'trabalho', 'pot√™ncia', 'newton', 'cin√©tica', 'potencial'],
            "Termodin√¢mica": ['temperatura', 'calor', 'termodin√¢mica', 'g√°s', 'press√£o'],
            "√ìptica": ['luz', 'lente', 'espelho', 'refra√ß√£o', 'reflex√£o', '√≥ptica'],
            "Ondulat√≥ria": ['onda', 'frequ√™ncia', 'amplitude', 'som', 'doppler'],
            "Eletricidade": ['corrente', 'tens√£o', 'resist√™ncia', 'circuito', 'el√©trons', 'eletricidade', 'eletrost√°tica'],
            # Qu√≠mica
            "Qu√≠mica Org√¢nica": ['carbono', 'hidrocarboneto', '√°lcool', 'fun√ß√£o org√¢nica'],
            "Estequiometria": ['mol', 'massa molar', 'estequiometria', 'c√°lculo estequiom√©trico'],
            "Solu√ß√µes": ['solu√ß√£o', 'concentra√ß√£o', 'molaridade', 'solubilidade'],
            "Termoqu√≠mica": ['entalpia', 'rea√ß√£o exot√©rmica', 'rea√ß√£o endot√©rmica'],
            "Eletroqu√≠mica": ['pilha', 'eletr√≥lise', 'oxida√ß√£o', 'redu√ß√£o'],
            # Biologia
            "Citologia": ['c√©lula', 'membrana', 'citoplasma', 'n√∫cleo', 'mitoc√¥ndria'],
            "Gen√©tica": ['gene', 'dna', 'hereditariedade', 'gen√©tica', 'mendel'],
            "Ecologia": ['ecossistema', 'bioma', 'cadeia alimentar', 'popula√ß√£o'],
            "Fisiologia Humana": ['sistema digest√≥rio', 'sistema respirat√≥rio', 'sistema circulat√≥rio']
        }
        
        all_topics = {**math_topics, **science_topics}
        
        for topic, keywords in all_topics.items():
            if any(keyword in text_lower for keyword in keywords):
                return topic
        
        return "Geral"
    
    def _create_vectorstore(self):
        """Cria e persiste o vectorstore com os documentos"""
        if not self.documents:
            st.warning("Nenhum documento para processar")
            return
            
        try:
            st.info(f"üíæ Criando vectorstore com {len(self.documents)} exerc√≠cios...")
            
            # Divide os documentos em chunks menores (se necess√°rio)
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=150
            )
            splits = text_splitter.split_documents(self.documents)
            
            self.vectorstore = Chroma.from_documents(
                documents=splits,
                embedding=self.embeddings,
                persist_directory=self.persist_directory
            )
            self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": 5})
            st.success("‚úÖ Vectorstore criado e salvo com sucesso!")
            
        except Exception as e:
            st.error(f"Erro ao criar vectorstore: {str(e)}")

    def load_existing_vectorstore(self) -> bool:
        """Carrega um vectorstore Chroma existente"""
        if self.vectorstore:
            return True
            
        if not os.path.exists(self.persist_directory):
            print(f"‚ö†Ô∏è Diret√≥rio de persist√™ncia n√£o encontrado: {self.persist_directory}")
            return False
            
        try:
            print(f"üìö Carregando vectorstore de '{self.persist_directory}'...")
            self.vectorstore = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embeddings
            )
            self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": 5})
            print("‚úÖ Vectorstore carregado com sucesso.")
            
            # Verifica se o vectorstore n√£o est√° vazio
            if not self.vectorstore._collection.count():
                 print("‚ö†Ô∏è Vectorstore est√° vazio. Recomenda-se reprocessar os documentos.")
                 return False

            return True
            
        except Exception as e:
            print(f"‚ùå Erro ao carregar vectorstore: {str(e)}")
            return False

    def search_exercises_by_message(self, message: str, k: int = 3) -> List[Dict[str, Any]]:
        """Busca exerc√≠cios por similaridade com a mensagem do usu√°rio."""
        if not self.vectorstore:
            # Tenta carregar o vectorstore se ele n√£o estiver na mem√≥ria
            if not self.load_existing_vectorstore():
                # Se n√£o conseguir carregar, processa os documentos para criar um novo
                print("Vectorstore n√£o encontrado. Processando documentos para criar um novo...")
                self.process_enem_documents()
                if not self.vectorstore:
                    print("‚ùå Falha ao criar ou carregar o vectorstore. A busca n√£o pode ser realizada.")
                    return []

        try:
            # Realiza a busca por similaridade
            results = self.vectorstore.similarity_search(message, k=k)
            
            # Formata os resultados para o padr√£o esperado
            formatted_results = []
            for doc in results:
                formatted_results.append({
                    "year": doc.metadata.get("year", "N/A"),
                    "question_number": doc.metadata.get("question_number", "N/A"),
                    "topic": doc.metadata.get("topic", "Geral"),
                    "content": doc.page_content
                })
            
            return formatted_results
        except Exception as e:
            print(f"Erro durante a busca por similaridade: {e}")
            return []
            
    def search_exercises_by_topic(self, topic: str, subject_area: str = None, k: int = 3) -> List[Document]:
        """Busca exerc√≠cios por t√≥pico, com filtro opcional de √°rea"""
        if not self.retriever:
            st.warning("Retriever n√£o inicializado")
            return []
        
        try:
            # Constr√≥i query
            query = topic
            if subject_area:
                query += f" {subject_area}"
            
            # Busca documentos relevantes
            docs = self.retriever.invoke(query)
            
            # Filtra por √°rea se especificada
            if subject_area:
                filtered_docs = []
                for doc in docs:
                    doc_area = doc.metadata.get("subject_area", "").lower()
                    if subject_area.lower() in doc_area or doc_area in subject_area.lower():
                        filtered_docs.append(doc)
                docs = filtered_docs
            
            return docs[:k]
            
        except Exception as e:
            print(f"Erro na busca de exerc√≠cios: {str(e)}")
            return []
    
    def get_exercises_by_year(self, year: str, k: int = 5) -> List[Document]:
        """Busca exerc√≠cios de um ano espec√≠fico"""
        if not self.vectorstore:
            return []
        
        try:
            # Busca por ano nos metadados
            docs = self.vectorstore.similarity_search(
                "quest√£o exerc√≠cio",
                k=k*3,  # Busca mais para filtrar por ano
                filter={"year": year}
            )
            
            return docs[:k]
            
        except Exception as e:
            print(f"Erro na busca por ano: {str(e)}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas dos exerc√≠cios"""
        stats = {
            "total_exercises": len(self.documents),
            "vectorstore_initialized": self.vectorstore is not None,
            "retriever_configured": self.retriever is not None,
            "enem_folder": str(self.enem_folder_path),
            "persistence_directory": self.persist_directory
        }
        
        # Conta anos dispon√≠veis
        years = set()
        areas = set()
        topics = set()
        
        for doc in self.documents:
            year = doc.metadata.get("year", "Desconhecido")
            area = doc.metadata.get("subject_area", "Desconhecido")
            topic = doc.metadata.get("topic", "Desconhecido")
            
            years.add(year)
            areas.add(area)
            topics.add(topic)
        
        stats["available_years"] = sorted(list(years), reverse=True)
        stats["subject_areas"] = sorted(list(areas))
        stats["topics"] = sorted(list(topics))
        
        return stats

# Inst√¢ncia global
enem_exercises_rag = ENEMExercisesRAG() 