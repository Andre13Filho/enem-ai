"""
Sistema RAG Local para Professor Carlos - Matem√°tica
Utiliza um √≠ndice FAISS pr√©-constru√≠do e baixado do Hugging Face.
"""

import streamlit as st
import os
import requests
from typing import Dict, List, Any, Optional

# LangChain imports
from langchain_community.vectorstores import FAISS
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from langchain.chains import ConversationalRetrievalChain
try:
    from langchain_community.memory import ConversationBufferMemory
except ImportError:
    from langchain.memory import ConversationBufferMemory
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun

# Groq para LLM
from groq import Groq

# Diret√≥rio para armazenar o √≠ndice FAISS
FAISS_INDEX_DIR = "faiss_index_math"

class GroqLLM(LLM):
    """LLM personalizado para DeepSeek R1 Distill via Groq"""
    
    api_key: str
    model_name: str = "deepseek-r1-distill-llama-70b"
    
    class Config:
        arbitrary_types_allowed = True
    
    def __init__(self, api_key: str, **kwargs):
        super().__init__(api_key=api_key, model_name="deepseek-r1-distill-llama-70b", **kwargs)
        self._client = Groq(api_key=api_key)
    
    @property
    def _llm_type(self) -> str:
        return "groq"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        try:
            response = self._client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=2048
            )
            return response.choices[0].message.content
        except Exception as e:
            from encoding_utils import safe_api_error
            return safe_api_error(e)

class LocalMathRAG:
    """Sistema RAG que carrega um √≠ndice FAISS remoto."""
    
    def __init__(self):
        self.vectorstore = None
        self.retriever = None
        self.memory = None
        self.rag_chain = None
        self.embeddings = None
        self.is_initialized = False
        
        # O modelo de embedding DEVE ser o mesmo usado na cria√ß√£o do √≠ndice
        self._setup_embeddings("sentence-transformers/distiluse-base-multilingual-cased-v1")
    
    def _setup_embeddings(self, model_name: str):
        """Configura embeddings usando HuggingFace"""
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception as e:
            st.error(f"Erro ao configurar embeddings: {str(e)}")
            print(f"‚ùå Erro ao configurar embeddings: {str(e)}")
            self.embeddings = None

    def _download_file(self, url: str, local_path: str):
        """Baixa um arquivo de uma URL para um caminho local."""
        try:
            with requests.get(url, stream=True) as r:
                r.raise_for_status()
                with open(local_path, 'wb') as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        f.write(chunk)
            print(f"‚úÖ Arquivo baixado: {local_path}")
            return True
        except requests.exceptions.RequestException as e:
            st.error(f"Erro de rede ao baixar {url}: {e}")
            print(f"‚ùå Erro de rede ao baixar {url}: {e}")
            return False
        
    def _ensure_faiss_index_is_ready(self) -> bool:
        """
        Garante que o √≠ndice FAISS esteja dispon√≠vel, baixando-o se necess√°rio.
        """
        os.makedirs(FAISS_INDEX_DIR, exist_ok=True)
        
        index_file = os.path.join(FAISS_INDEX_DIR, "index.faiss")
        pkl_file = os.path.join(FAISS_INDEX_DIR, "index.pkl")

        # Verifica se os dois arquivos j√° existem
        if os.path.exists(index_file) and os.path.exists(pkl_file):
            print("‚úÖ √çndice FAISS j√° existe localmente.")
            return True

        st.info("üì• Baixando √≠ndice de matem√°tica do Hugging Face...")
        print("üì• Baixando √≠ndice de matem√°tica do Hugging Face...")

        # URLs dos arquivos no Hugging Face
        faiss_url = "https://huggingface.co/Andre13Filho/rag_enem/resolve/main/index.faiss"
        pkl_url = "https://huggingface.co/Andre13Filho/rag_enem/resolve/main/index.pkl"

        # Baixa os dois arquivos
        faiss_success = self._download_file(faiss_url, index_file)
        pkl_success = self._download_file(pkl_url, pkl_file)

        if faiss_success and pkl_success:
            st.success("‚úÖ √çndice de matem√°tica baixado com sucesso!")
            return True
        else:
            st.error("‚ùå Falha ao baixar os arquivos do √≠ndice de matem√°tica.")
            # Limpa arquivos parciais em caso de falha
            if os.path.exists(index_file): os.remove(index_file)
            if os.path.exists(pkl_file): os.remove(pkl_file)
            return False
            
    def initialize(self, api_key: str) -> bool:
        """
        Inicializa o sistema: baixa o √≠ndice, carrega o vectorstore e cria a cadeia RAG.
        """
        if self.is_initialized:
            return True
            
        if not self.embeddings:
            st.error("Embeddings n√£o foram inicializadas. Abortando.")
            return False
    
        # 1. Garantir que o √≠ndice FAISS est√° dispon√≠vel
        if not self._ensure_faiss_index_is_ready():
            return False
            
        # 2. Carregar o Vectorstore FAISS
        try:
            st.info("üìö Carregando base de conhecimento de matem√°tica (FAISS)...")
            print("üìö Carregando base de conhecimento de matem√°tica (FAISS)...")
            self.vectorstore = FAISS.load_local(
                FAISS_INDEX_DIR, 
                self.embeddings,
                allow_dangerous_deserialization=True # Necess√°rio para pkl
            )
            self.retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 5}
            )
            st.success("‚úÖ Base de conhecimento carregada.")
            print("‚úÖ Base de conhecimento carregada.")
        except Exception as e:
            st.error(f"Erro ao carregar o √≠ndice FAISS: {e}")
            print(f"‚ùå Erro ao carregar o √≠ndice FAISS: {e}")
            return False
    
        # 3. Criar a cadeia RAG
        try:
            st.info("üîó Criando a cadeia de conversa√ß√£o RAG...")
            print("üîó Criando a cadeia de conversa√ß√£o RAG...")
            self.memory = ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
                output_key="answer"
            )
        
            llm = GroqLLM(api_key=api_key)

            self.rag_chain = ConversationalRetrievalChain.from_llm(
                llm=llm,
                retriever=self.retriever,
                memory=self.memory,
                return_source_documents=True,
                output_key="answer",
            )
            
            # Adiciona o prompt personalizado para formata√ß√£o de LaTeX
            prompt_template = """Voc√™ √© o Professor Carlos, especialista em matem√°tica do ENEM. Responda como um professor para uma estudante de 17 anos.

REGRAS DE FORMATA√á√ÉO (OBRIGAT√ìRIO):
1. Use APENAS os delimitadores de LaTeX do MathJax para f√≥rmulas.
2. Para f√≥rmulas no meio do texto (inline), use um √∫nico cifr√£o: $sua-formula-aqui$.
3. Para f√≥rmulas em destaque (display), use dois cifr√µes: $$sua-formula-aqui$$.
4. NUNCA use colchetes `[ ]` ou `( )` para delimitar f√≥rmulas.
5. Use `\\text{...}` para texto dentro de f√≥rmulas. Exemplo: `$$A_{\\text{c√≠rculo}} = \\pi r^2$$`.

Com base no CONTEXTO abaixo, responda √† PERGUNTA do aluno.
Se a resposta n√£o estiver no contexto, use seu conhecimento em matem√°tica, mas mantenha o estilo.

CONTEXTO:
{context}

PERGUNTA: {question}

RESPOSTA:
"""
            # Atualiza o prompt da cadeia
            if hasattr(self.rag_chain.combine_docs_chain, "llm_chain"):
                self.rag_chain.combine_docs_chain.llm_chain.prompt.template = prompt_template
                
            st.success("‚úÖ Cadeia RAG pronta!")
            print("‚úÖ Cadeia RAG pronta!")
            self.is_initialized = True
            return True
        except Exception as e:
            st.error(f"Erro ao criar a cadeia RAG: {e}")
            print(f"‚ùå Erro ao criar a cadeia RAG: {e}")
            return False
    
    def get_response(self, question: str) -> Dict[str, Any]:
        """Obt√©m uma resposta do sistema RAG."""
        if not self.rag_chain:
            return {"answer": "O sistema RAG n√£o foi inicializado corretamente."}
        
        try:
            return self.rag_chain({"question": question})
        except Exception as e:
            return {"answer": f"Erro ao processar a pergunta: {str(e)}"}
    
    def search_relevant_content(self, query: str, k: int = 3) -> List[Document]:
        """Busca por conte√∫do relevante no vectorstore."""
        if not self.vectorstore:
            return []
        
        try:
            return self.vectorstore.similarity_search(query, k=k)
        except Exception as e:
            print(f"Erro na busca de similaridade: {str(e)}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Retorna estat√≠sticas detalhadas do sistema RAG, incluindo uma amostra de documentos.
        """
        if not self.is_initialized or not self.vectorstore:
            return {
                "status": "N√£o Carregado",
                "total_documents": 0,
                "sample_documents": []
            }

        try:
            total_documents = self.vectorstore.index.ntotal
            
            # Pega uma amostra de metadados dos primeiros 5 documentos
            sample_docs_metadata = []
            docstore = self.vectorstore.docstore
            doc_ids = list(docstore._dict.keys())
            
            for i in range(min(5, len(doc_ids))):
                doc = docstore._dict[doc_ids[i]]
                if doc.metadata:
                    sample_docs_metadata.append(doc.metadata)

            # Extrai nomes de arquivos √∫nicos da amostra
            sample_files = sorted(list(set(
                meta.get("source", "Fonte Desconhecida") for meta in sample_docs_metadata
            )))

            return {
                "status": "Carregado",
                "total_documents": total_documents,
                "sample_documents": sample_files
            }
        except Exception as e:
            print(f"Erro ao obter estat√≠sticas do RAG: {e}")
            return {
                "status": "Erro na Leitura",
                "total_documents": 0,
                "sample_documents": [str(e)]
            }
    
    def clear_memory(self):
        """Limpa a mem√≥ria da conversa."""
        if self.memory:
            self.memory.clear()

# Singleton instance
local_math_rag = LocalMathRAG() 