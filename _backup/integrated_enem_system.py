#!/usr/bin/env python3
"""
Sistema Integrado do ENEM - Combina ExtraÃ§Ã£o AvanÃ§ada de PDF + Parser Melhorado
VersÃ£o final que resolve completamente os problemas de alternativas bagunÃ§adas
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import logging
import streamlit as st

# Imports dos sistemas desenvolvidos
from advanced_pdf_extractor import AdvancedPDFExtractor, extract_pdf_advanced
from improved_exercise_parser import ImprovedExerciseParser, ExerciseQuestion
from improved_exercise_display import ImprovedExerciseDisplay, display_exercises_list

# LangChain para vectorstore
from langchain.schema import Document
from langchain_chroma import Chroma
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings

logger = logging.getLogger(__name__)

class IntegratedENEMSystem:
    """
    Sistema integrado que combina:
    1. ExtraÃ§Ã£o avanÃ§ada de PDF (PyMuPDF + OCR)
    2. Parser melhorado com estruturaÃ§Ã£o JSON
    3. RAG otimizado
    4. Interface moderna
    """
    
    def __init__(self, enem_folder_path: str = "./Segundo dia"):
        self.enem_folder_path = Path(enem_folder_path)
        self.persist_directory = "./chroma_enem_integrated"
        
        # Componentes do sistema
        self.pdf_extractor = AdvancedPDFExtractor()
        self.exercise_parser = ImprovedExerciseParser()
        self.exercise_display = ImprovedExerciseDisplay()
        
        # ConfiguraÃ§Ãµes RAG
        self.embeddings = None
        self.vectorstore = None
        self.retriever = None
        
        # Cache de exercÃ­cios processados
        self.structured_exercises = []
        self.processing_stats = {}
        
        self._setup_embeddings()
    
    def _setup_embeddings(self):
        """Configura embeddings para RAG"""
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception as e:
            logger.error(f"Erro ao configurar embeddings: {e}")
    
    def process_enem_documents_advanced(self) -> bool:
        """
        Processamento completo usando o pipeline integrado:
        PDF AvanÃ§ado â†’ Parser Melhorado â†’ RAG Otimizado
        """
        logger.info("ğŸš€ Iniciando processamento integrado do ENEM")
        
        if not self.enem_folder_path.exists():
            logger.error(f"Pasta do ENEM nÃ£o encontrada: {self.enem_folder_path}")
            return False
        
        try:
            total_exercises = 0
            processing_details = {
                "files_processed": 0,
                "extraction_methods": {},
                "ocr_pages": 0,
                "parsing_quality": {},
                "errors": []
            }
            
            # Busca todos os anos
            year_folders = [f for f in self.enem_folder_path.iterdir() if f.is_dir()]
            year_folders.sort(key=lambda x: x.name, reverse=True)
            
            for year_folder in year_folders:
                year = year_folder.name
                logger.info(f"ğŸ“… Processando ano {year}...")
                
                # Busca PDFs de prova
                pdf_files = [f for f in year_folder.glob("*.pdf") 
                           if "gabarito" not in f.name.lower() and "gb" not in f.name.lower()]
                
                for pdf_file in pdf_files:
                    logger.info(f"   ğŸ“„ Processando: {pdf_file.name}")
                    
                    try:
                        # ETAPA 1: ExtraÃ§Ã£o avanÃ§ada de PDF
                        structured_text, extraction_stats = extract_pdf_advanced(pdf_file)
                        
                        # Atualiza estatÃ­sticas de extraÃ§Ã£o
                        method = extraction_stats["extraction_method"]
                        processing_details["extraction_methods"][method] = \
                            processing_details["extraction_methods"].get(method, 0) + 1
                        
                        if extraction_stats.get("ocr_enabled", False):
                            processing_details["ocr_pages"] += extraction_stats.get("total_pages", 0)
                        
                        # ETAPA 2: Parser melhorado com estruturaÃ§Ã£o JSON
                        exercises = self.exercise_parser.parse_pdf_text_to_exercises(
                            structured_text, year, pdf_file.name
                        )
                        
                        if exercises:
                            self.structured_exercises.extend(exercises)
                            total_exercises += len(exercises)
                            processing_details["files_processed"] += 1
                            
                            # Calcula qualidade do parsing
                            quality_scores = [self._calculate_exercise_quality(ex) for ex in exercises]
                            avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
                            processing_details["parsing_quality"][pdf_file.name] = avg_quality
                            
                            logger.info(f"   âœ… {len(exercises)} exercÃ­cios estruturados extraÃ­dos (qualidade: {avg_quality:.1f}%)")
                        else:
                            logger.warning(f"   âš ï¸ Nenhum exercÃ­cio extraÃ­do de {pdf_file.name}")
                    
                    except Exception as e:
                        error_msg = f"Erro ao processar {pdf_file.name}: {str(e)}"
                        logger.error(f"   âŒ {error_msg}")
                        processing_details["errors"].append(error_msg)
            
            # ETAPA 3: CriaÃ§Ã£o do RAG otimizado
            if self.structured_exercises:
                self._create_integrated_vectorstore()
                self._save_processing_cache()
                
                self.processing_stats = processing_details
                self.processing_stats["total_exercises"] = total_exercises
                
                logger.info(f"âœ… Processamento integrado concluÃ­do: {total_exercises} exercÃ­cios")
                return True
            else:
                logger.warning("âŒ Nenhum exercÃ­cio foi extraÃ­do")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erro no processamento integrado: {e}")
            return False
    
    def _calculate_exercise_quality(self, exercise: ExerciseQuestion) -> float:
        """Calcula qualidade de um exercÃ­cio (0-100)"""
        score = 0
        
        # Enunciado (40 pontos)
        if len(exercise.enunciado) >= 100:
            score += 40
        elif len(exercise.enunciado) >= 50:
            score += 20
        
        # Alternativas (40 pontos)
        if len(exercise.alternativas) >= 5:
            score += 20
        elif len(exercise.alternativas) >= 3:
            score += 10
        
        valid_alternatives = sum(1 for alt in exercise.alternativas if alt.is_valid)
        if valid_alternatives >= 4:
            score += 20
        elif valid_alternatives >= 3:
            score += 15
        elif valid_alternatives >= 2:
            score += 10
        
        # Metadados (20 pontos)
        if exercise.topicos_chave:
            score += 10
        if exercise.dificuldade_estimada and exercise.dificuldade_estimada != "":
            score += 10
        
        return min(100, score)
    
    def _create_integrated_vectorstore(self):
        """Cria vectorstore otimizado com dados estruturados"""
        try:
            documents = []
            
            for exercise in self.structured_exercises:
                # ConteÃºdo otimizado para busca
                search_content = self._create_enhanced_search_content(exercise)
                
                # Metadados super enriquecidos
                metadata = {
                    "id_questao": exercise.id_questao,
                    "year": exercise.ano,
                    "question_number": exercise.numero_questao,
                    "subject_area": exercise.area_conhecimento,
                    "topics": exercise.topicos_chave,
                    "difficulty": exercise.dificuldade_estimada,
                    "skill": exercise.habilidade_associada,
                    "source_file": exercise.fonte_arquivo,
                    "document_type": "integrated_exercise",
                    "quality_score": self._calculate_exercise_quality(exercise),
                    "num_alternatives": len(exercise.alternativas),
                    "valid_alternatives": sum(1 for alt in exercise.alternativas if alt.is_valid),
                    "avg_confidence": sum(alt.confidence for alt in exercise.alternativas) / len(exercise.alternativas) if exercise.alternativas else 0,
                    "has_structured_data": True
                }
                
                doc = Document(page_content=search_content, metadata=metadata)
                documents.append(doc)
            
            # Cria vectorstore
            self.vectorstore = Chroma.from_documents(
                documents=documents,
                embedding=self.embeddings,
                persist_directory=self.persist_directory
            )
            
            self.retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 5}
            )
            
            logger.info(f"âœ… Vectorstore integrado criado com {len(documents)} exercÃ­cios")
            
        except Exception as e:
            logger.error(f"Erro ao criar vectorstore integrado: {e}")
    
    def _create_enhanced_search_content(self, exercise: ExerciseQuestion) -> str:
        """Cria conteÃºdo super otimizado para busca semÃ¢ntica"""
        content_parts = []
        
        # Metadados para busca
        content_parts.append(f"QUESTÃƒO {exercise.numero_questao} ENEM {exercise.ano}")
        content_parts.append(f"ÃREA: {exercise.area_conhecimento}")
        
        if exercise.topicos_chave:
            content_parts.append(f"TÃ“PICOS: {' '.join(exercise.topicos_chave)}")
        
        content_parts.append(f"DIFICULDADE: {exercise.dificuldade_estimada}")
        content_parts.append(f"HABILIDADE: {exercise.habilidade_associada}")
        
        # Enunciado limpo
        content_parts.append("ENUNCIADO:")
        cleaned_statement = exercise.enunciado.replace("QUESTÃƒO", "").strip()
        content_parts.append(cleaned_statement)
        
        # Alternativas vÃ¡lidas apenas
        valid_alternatives = [alt for alt in exercise.alternativas if alt.is_valid and alt.texto.strip()]
        if valid_alternatives:
            content_parts.append("ALTERNATIVAS:")
            for alt in valid_alternatives:
                content_parts.append(f"{alt.letra}) {alt.texto}")
        
        # Palavras-chave para busca
        keywords = []
        for topic in exercise.topicos_chave:
            keywords.extend(topic.split())
        
        if keywords:
            content_parts.append(f"PALAVRAS-CHAVE: {' '.join(set(keywords))}")
        
        return "\n".join(content_parts)
    
    def _save_processing_cache(self):
        """Salva cache do processamento integrado"""
        try:
            cache_file = Path(self.persist_directory) / "integrated_cache.json"
            cache_file.parent.mkdir(exist_ok=True)
            
            cache_data = {
                "exercises": [ex.to_dict() for ex in self.structured_exercises],
                "processing_stats": self.processing_stats,
                "version": "integrated_v1.0"
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… Cache integrado salvo: {len(self.structured_exercises)} exercÃ­cios")
            
        except Exception as e:
            logger.error(f"Erro ao salvar cache integrado: {e}")
    
    def load_existing_system(self) -> bool:
        """Carrega sistema integrado existente"""
        try:
            if Path(self.persist_directory).exists():
                # Carrega vectorstore
                self.vectorstore = Chroma(
                    persist_directory=self.persist_directory,
                    embedding_function=self.embeddings
                )
                self.retriever = self.vectorstore.as_retriever(
                    search_type="similarity",
                    search_kwargs={"k": 5}
                )
                
                # Carrega cache
                cache_file = Path(self.persist_directory) / "integrated_cache.json"
                if cache_file.exists():
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        cache_data = json.load(f)
                    
                    # ReconstrÃ³i exercÃ­cios
                    self.structured_exercises = []
                    for ex_data in cache_data.get("exercises", []):
                        exercise = self._rebuild_exercise_from_dict(ex_data)
                        if exercise:
                            self.structured_exercises.append(exercise)
                    
                    self.processing_stats = cache_data.get("processing_stats", {})
                    
                    logger.info(f"âœ… Sistema integrado carregado: {len(self.structured_exercises)} exercÃ­cios")
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"Erro ao carregar sistema integrado: {e}")
            return False
    
    def _rebuild_exercise_from_dict(self, data: Dict) -> Optional[ExerciseQuestion]:
        """ReconstrÃ³i ExerciseQuestion a partir do cache"""
        try:
            from improved_exercise_parser import Alternative
            
            alternatives = [Alternative(**alt_data) for alt_data in data.get('alternativas', [])]
            
            exercise = ExerciseQuestion(
                id_questao=data['id_questao'],
                area_conhecimento=data['area_conhecimento'],
                enunciado=data['enunciado'],
                alternativas=alternatives,
                alternativa_correta=data.get('alternativa_correta'),
                topicos_chave=data.get('topicos_chave', []),
                ano=data.get('ano', ''),
                numero_questao=data.get('numero_questao', ''),
                fonte_arquivo=data.get('fonte_arquivo', ''),
                habilidade_associada=data.get('habilidade_associada', ''),
                dificuldade_estimada=data.get('dificuldade_estimada', ''),
                tipo_questao=data.get('tipo_questao', 'mÃºltipla escolha')
            )
            
            return exercise
            
        except Exception as e:
            logger.error(f"Erro ao reconstruir exercÃ­cio: {e}")
            return None
    
    def search_exercises_advanced(self, query: str, filters: Dict[str, Any] = None, k: int = 5) -> List[ExerciseQuestion]:
        """
        Busca avanÃ§ada de exercÃ­cios com filtros mÃºltiplos
        """
        if not self.retriever:
            return []
        
        try:
            # Busca base
            docs = self.retriever.get_relevant_documents(query)
            
            # Aplica filtros
            if filters:
                filtered_docs = []
                for doc in docs:
                    metadata = doc.metadata
                    
                    # Filtros disponÃ­veis
                    if filters.get("subject_area") and filters["subject_area"].lower() not in metadata.get("subject_area", "").lower():
                        continue
                    
                    if filters.get("difficulty") and filters["difficulty"].lower() != metadata.get("difficulty", "").lower():
                        continue
                    
                    if filters.get("year") and str(filters["year"]) != str(metadata.get("year", "")):
                        continue
                    
                    if filters.get("min_quality") and metadata.get("quality_score", 0) < filters["min_quality"]:
                        continue
                    
                    if filters.get("topics"):
                        doc_topics = metadata.get("topics", [])
                        if not any(topic.lower() in [t.lower() for t in doc_topics] for topic in filters["topics"]):
                            continue
                    
                    filtered_docs.append(doc)
                
                docs = filtered_docs
            
            # Limita resultados
            docs = docs[:k]
            
            # Mapeia para exercÃ­cios estruturados
            results = []
            for doc in docs:
                question_id = doc.metadata.get("id_questao", "")
                exercise = self._find_exercise_by_id(question_id)
                if exercise:
                    results.append(exercise)
            
            return results
            
        except Exception as e:
            logger.error(f"Erro na busca avanÃ§ada: {e}")
            return []
    
    def _find_exercise_by_id(self, question_id: str) -> Optional[ExerciseQuestion]:
        """Encontra exercÃ­cio pelo ID"""
        for exercise in self.structured_exercises:
            if exercise.id_questao == question_id:
                return exercise
        return None
    
    def get_system_stats(self) -> Dict[str, Any]:
        """EstatÃ­sticas completas do sistema integrado"""
        base_stats = {
            "total_exercises": len(self.structured_exercises),
            "processing_stats": self.processing_stats,
            "system_version": "integrated_v1.0"
        }
        
        if self.structured_exercises:
            # EstatÃ­sticas por Ã¡rea
            areas = {}
            difficulties = {}
            years = {}
            
            for ex in self.structured_exercises:
                areas[ex.area_conhecimento] = areas.get(ex.area_conhecimento, 0) + 1
                difficulties[ex.dificuldade_estimada] = difficulties.get(ex.dificuldade_estimada, 0) + 1
                years[ex.ano] = years.get(ex.ano, 0) + 1
            
            # Qualidade mÃ©dia
            quality_scores = [self._calculate_exercise_quality(ex) for ex in self.structured_exercises]
            avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
            
            base_stats.update({
                "by_area": areas,
                "by_difficulty": difficulties,
                "by_year": years,
                "average_quality": avg_quality,
                "high_quality_exercises": sum(1 for score in quality_scores if score >= 80),
                "components": {
                    "pdf_extractor": "AdvancedPDFExtractor",
                    "exercise_parser": "ImprovedExerciseParser", 
                    "display_system": "ImprovedExerciseDisplay",
                    "vectorstore": "Chroma + HuggingFace"
                }
            })
        
        return base_stats
    
    def display_exercises_with_advanced_ui(self, exercises: List[ExerciseQuestion], title: str = "ğŸ“ ExercÃ­cios Encontrados"):
        """Exibe exercÃ­cios usando a interface moderna"""
        display_exercises_list(exercises, title)

# InstÃ¢ncia global do sistema integrado
integrated_enem_system = IntegratedENEMSystem()

if __name__ == "__main__":
    # Teste do sistema integrado
    print("ğŸš€ Testando Sistema Integrado do ENEM")
    print("=" * 60)
    
    system = IntegratedENEMSystem()
    
    # Tenta carregar sistema existente
    if system.load_existing_system():
        print("âœ… Sistema integrado carregado do cache")
        stats = system.get_system_stats()
        print(f"ğŸ“Š EstatÃ­sticas: {json.dumps(stats, indent=2, ensure_ascii=False)}")
        
        # Teste de busca
        results = system.search_exercises_advanced("energia cinÃ©tica", k=2)
        if results:
            print(f"\nğŸ” Teste de busca: {len(results)} exercÃ­cios encontrados")
            for ex in results:
                print(f"   ğŸ“ {ex.id_questao} - {ex.area_conhecimento}")
    else:
        print("âš ï¸ Sistema integrado nÃ£o encontrado. Execute o processamento primeiro.")
        print("ğŸ’¡ Use: system.process_enem_documents_advanced()") 